{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a980da20",
   "metadata": {},
   "source": [
    "# Snowflake — 15 Medium Programming Questions & Detailed Answers\n",
    "\n",
    "This document contains 15 medium-difficulty Snowflake programming questions with clear, practical solutions including SQL examples, Snowflake-specific features, and short explanations.\n",
    "\n",
    "---\n",
    "\n",
    "## Q1 — Load semi-structured JSON from stage and extract nested fields\n",
    "\n",
    "**Problem**\n",
    "- You have newline-delimited JSON files in an internal stage. Each record has nested objects and arrays. Load into a table with extracted fields and preserve the variant.\n",
    "\n",
    "**Solution**\n",
    "- Create table and file format, then `COPY INTO` using the `VARIANT` column and `SELECT` to extract nested fields:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE FILE FORMAT my_json_fmt TYPE = 'JSON' STRIP_OUTER_ARRAY = FALSE;\n",
    "\n",
    "CREATE OR REPLACE TABLE orders_raw (raw VARIANT);\n",
    "\n",
    "COPY INTO orders_raw\n",
    "  FROM @my_stage/path/\n",
    "  FILE_FORMAT = (FORMAT_NAME = 'my_json_fmt')\n",
    "  ON_ERROR = 'CONTINUE';\n",
    "\n",
    "-- Extract nested fields\n",
    "CREATE OR REPLACE TABLE orders AS\n",
    "SELECT\n",
    "  raw:id::STRING AS order_id,\n",
    "  raw:customer.id::STRING AS customer_id,\n",
    "  raw:customer.name::STRING AS customer_name,\n",
    "  raw:items AS items_variant,\n",
    "  raw:total::NUMBER AS total\n",
    "FROM orders_raw;\n",
    "\n",
    "-- Flatten items array\n",
    "SELECT\n",
    "  o.order_id,\n",
    "  f.value:sku::STRING AS sku,\n",
    "  f.value:qty::NUMBER AS qty\n",
    "FROM orders o,\n",
    "LATERAL FLATTEN(input => o.items_variant) f;\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- Use a `VARIANT` column to store raw JSON, then type-cast paths using the colon syntax. `LATERAL FLATTEN` expands arrays. Keep raw variant for auditing.\n",
    "\n",
    "---\n",
    "\n",
    "## Q2 — Implement slowly changing dimension (SCD Type 2) using MERGE\n",
    "\n",
    "**Problem**\n",
    "- Maintain `dim_customer` as SCD Type 2 with effective start/end timestamps when loading a changed records file.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "```sql\n",
    "-- staging_customer has incoming snapshot with natural_key and attributes\n",
    "MERGE INTO dim_customer T\n",
    "USING (\n",
    "  SELECT *, CURRENT_TIMESTAMP() AS load_ts FROM staging_customer\n",
    ") S\n",
    "ON T.natural_key = S.natural_key AND T.end_ts IS NULL\n",
    "WHEN MATCHED AND (\n",
    "     (T.attr1 IS DISTINCT FROM S.attr1) OR\n",
    "     (T.attr2 IS DISTINCT FROM S.attr2)\n",
    "  ) THEN\n",
    "  UPDATE SET end_ts = S.load_ts\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (natural_key, attr1, attr2, start_ts, end_ts)\n",
    "  VALUES (S.natural_key, S.attr1, S.attr2, S.load_ts, NULL);\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- Use `MERGE` to close existing current rows by setting `end_ts`. Then insert new current rows. `IS DISTINCT FROM` handles NULL-aware comparisons.\n",
    "\n",
    "---\n",
    "\n",
    "## Q3 — Stream + Task pattern: incremental ingest from staged files\n",
    "\n",
    "**Problem**\n",
    "- Use Snowflake Streams and Tasks to apply new staged CSV files (COPY INTO staging table) into a target table incrementally.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "1. Create a staging table and load raw files with COPY INTO. Create a stream on staging.\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE TABLE raw_events (payload VARIANT, src_file STRING, load_ts TIMESTAMP);\n",
    "CREATE OR REPLACE STREAM raw_events_stream ON TABLE raw_events;\n",
    "\n",
    "-- task that runs COPY into raw_events periodically (or triggered externally)\n",
    "CREATE OR REPLACE TASK task_load_raw\n",
    "  WAREHOUSE = my_wh\n",
    "  SCHEDULE = 'USING CRON 0 * * * * UTC'\n",
    "AS\n",
    "  COPY INTO raw_events FROM @my_stage/ FILE_FORMAT = (TYPE = 'CSV');\n",
    "\n",
    "-- Task that processes the stream into curated table\n",
    "CREATE OR REPLACE TASK task_process_stream\n",
    "  WAREHOUSE = my_wh\n",
    "  AFTER task_load_raw\n",
    "AS\n",
    "  INSERT INTO events SELECT payload:col1::STRING, payload:col2::NUMBER FROM raw_events_stream WHERE METADATA$ISROWDELETED = FALSE;\n",
    "\n",
    "-- enable tasks\n",
    "ALTER TASK task_load_raw RESUME;\n",
    "ALTER TASK task_process_stream RESUME;\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- Streams capture DML changes on the staging table; Tasks run scheduled pipelines. Use `METADATA$ISROWDELETED` to detect inserts vs deletes (if needed).\n",
    "\n",
    "---\n",
    "\n",
    "## Q4 — Efficiently retrieve top-N per group using window functions\n",
    "\n",
    "**Problem**\n",
    "- Return top 3 orders by amount per customer from `orders` table.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "```sql\n",
    "SELECT order_id, customer_id, total\n",
    "FROM (\n",
    "  SELECT order_id, customer_id, total,\n",
    "         ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY total DESC) AS rn\n",
    "  FROM orders\n",
    ")\n",
    "WHERE rn <= 3;\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- Use `ROW_NUMBER()` partitioned by customer and order by total descending. This pattern is efficient and idiomatic.\n",
    "\n",
    "---\n",
    "\n",
    "## Q5 — Querying semi-structured data with conditional logic\n",
    "\n",
    "**Problem**\n",
    "- JSON `events` column has variable fields. Compute `event_type` with fallback logic and extract nested value when present.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  e:id::STRING AS id,\n",
    "  COALESCE(e:type::STRING, e:meta.type::STRING, 'unknown') AS event_type,\n",
    "  CASE\n",
    "    WHEN e:event.payload IS NOT NULL THEN e:event.payload:data::STRING\n",
    "    ELSE NULL\n",
    "  END AS payload_data\n",
    "FROM raw_events;\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- Use `COALESCE` + variant path extraction. Use `CASE` to avoid runtime errors accessing absent paths.\n",
    "\n",
    "---\n",
    "\n",
    "## Q6 — Time travel and cloning for safe data repair\n",
    "\n",
    "**Problem**\n",
    "- Accidentally deleted rows; restore state from time-travel to a new table for comparison and recovery.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "```sql\n",
    "-- create clone from 1 hour ago\n",
    "CREATE TABLE orders_restore AS\n",
    "  SELECT * FROM orders AT (TIMESTAMP => DATEADD(hour, -1, CURRENT_TIMESTAMP()));\n",
    "\n",
    "-- compare or merge back missing rows\n",
    "MERGE INTO orders T\n",
    "USING orders_restore S\n",
    "ON T.order_id = S.order_id\n",
    "WHEN NOT MATCHED THEN INSERT *;\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- Time travel allows querying past data; cloning into a new table is zero-copy and fast. Use `MERGE` to recover missing rows.\n",
    "\n",
    "---\n",
    "\n",
    "## Q7 — Implement a stored procedure in Snowflake using Snowpark Python to deduplicate and write results\n",
    "\n",
    "**Problem**\n",
    "- Use Snowpark Python procedure to deduplicate a table by natural key and write the deduped result into a target table.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE PROCEDURE dedupe_table(src_table STRING, dst_table STRING)\n",
    "  RETURNS STRING\n",
    "  LANGUAGE PYTHON\n",
    "  RUNTIME_VERSION = '3.11'\n",
    "  PACKAGES = ('snowflake-snowpark-python')\n",
    "AS\n",
    "$$\n",
    "def run(session, src_table, dst_table):\n",
    "    df = session.table(src_table)\n",
    "    # keep latest by event_ts\n",
    "    windowed = df.with_column('rn', F.row_number().over(F.Window.partition_by('natural_key').order_by(F.col('event_ts').desc())))\n",
    "    deduped = windowed.filter(F.col('rn') == 1).drop('rn')\n",
    "    deduped.write.save_as_table(dst_table, mode='overwrite')\n",
    "    return 'OK'\n",
    "$$;\n",
    "\n",
    "CALL dedupe_table('raw_src', 'curated_dst');\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- Snowpark Python lets you express complex logic with DataFrame APIs server-side. Use window functions and save result to a table.\n",
    "\n",
    "---\n",
    "\n",
    "## Q8 — Optimize large table joins: clustering keys and pruning\n",
    "\n",
    "**Problem**\n",
    "- Joining a very large fact table to dimension filtered by date range is slow. Suggest improvements.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "**Guidance:**\n",
    "- Use proper clustering on the fact table (e.g., `CLUSTER BY (event_date)`) to improve pruning.\n",
    "- Ensure filters are on clustered columns. Use micro-partition pruning (verified via `QUERY_HISTORY`/`QUERY_PROFILE`).\n",
    "- Consider using a materialized view for frequently-run filtered aggregation.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```sql\n",
    "ALTER TABLE fact_events CLUSTER BY (event_date);\n",
    "\n",
    "CREATE MATERIALIZED VIEW mv_daily AS\n",
    "SELECT event_date, COUNT(*) AS cnt FROM fact_events GROUP BY event_date;\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- Clustering improves physical pruning; materialized views precompute aggregates.\n",
    "\n",
    "---\n",
    "\n",
    "## Q9 — Use `FLATTEN` + windowing to compute metrics across nested arrays\n",
    "\n",
    "**Problem**\n",
    "- Each `session` record contains an array of `actions`. Compute total actions and top action per session.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "```sql\n",
    "SELECT s.session_id,\n",
    "       COUNT(a.value) AS total_actions,\n",
    "       ARRAY_AGG(a.value ORDER BY a.value_count DESC)[0] AS top_action\n",
    "FROM (\n",
    "  SELECT session_id, f.value AS action\n",
    "  FROM sessions,\n",
    "       LATERAL FLATTEN(input => sessions.actions) f\n",
    ") a\n",
    "GROUP BY s.session_id;\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- Flatten then aggregate. For more complex top-K use `ROW_NUMBER()` over partition.\n",
    "\n",
    "---\n",
    "\n",
    "## Q10 — COPY performance tuning for bulk CSV loads\n",
    "\n",
    "**Problem**\n",
    "- Loading multi-GB CSV files to Snowflake is slow. What settings and patterns improve speed?\n",
    "\n",
    "**Solution**\n",
    "\n",
    "**Recommendations:**\n",
    "- Compress files (gzip) before staging.\n",
    "- Increase `MAX_CONCURRENCY_LEVEL` and use a larger warehouse size for COPY.\n",
    "- Use `PURGE = TRUE` only after successful loads to avoid repeated processing.\n",
    "- Use consistent `FILE_FORMAT` settings (e.g., `SKIP_HEADER`, `FIELD_OPTIONALLY_ENCLOSED_BY`).\n",
    "\n",
    "**Example COPY:**\n",
    "\n",
    "```sql\n",
    "COPY INTO my_table\n",
    "FROM @my_stage/bulk/\n",
    "FILE_FORMAT = (type='CSV' compression='GZIP' field_delimiter=',' skip_header=1)\n",
    "ON_ERROR='ABORT_STATEMENT'\n",
    "MAX_CONCURRENCY_LEVEL = 8;\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- Compression reduces network IO. Larger warehouses and parallelism speed ingestion.\n",
    "\n",
    "---\n",
    "\n",
    "## Q11 — Implement multi-table atomic update with transactions\n",
    "\n",
    "**Problem**\n",
    "- Update multiple related tables atomically (if any update fails, rollback all changes).\n",
    "\n",
    "**Solution**\n",
    "\n",
    "```sql\n",
    "BEGIN TRANSACTION;\n",
    "  UPDATE acct_balances SET balance = balance - 100 WHERE acct_id = 1;\n",
    "  UPDATE acct_balances SET balance = balance + 100 WHERE acct_id = 2;\n",
    "  INSERT INTO transfers (from_acct, to_acct, amount, ts) VALUES (1,2,100,CURRENT_TIMESTAMP());\n",
    "COMMIT;\n",
    "\n",
    "-- In case of error, use ROLLBACK;\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- Snowflake supports multi-statement transactions; use `BEGIN`/`COMMIT` to ensure atomicity.\n",
    "\n",
    "---\n",
    "\n",
    "## Q12 — Use `QUALIFY` to simplify filtering on analytic results\n",
    "\n",
    "**Problem**\n",
    "- Filter rows by the output of a window function (e.g., top 1 per group) without subquery nesting.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "```sql\n",
    "SELECT order_id, customer_id, total,\n",
    "       ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY total DESC) AS rn\n",
    "FROM orders\n",
    "QUALIFY rn = 1;\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- `QUALIFY` lets you filter on analytic/window expressions directly, reducing subquery complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## Q13 — Use `MERGE` to implement upsert with delete semantics (synchronization)\n",
    "\n",
    "**Problem**\n",
    "- Synchronize target table to match source: insert new, update changed, and delete target rows not present in source.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "```sql\n",
    "MERGE INTO target T\n",
    "USING source S\n",
    "ON T.key = S.key\n",
    "WHEN MATCHED THEN UPDATE SET T.col1 = S.col1, T.col2 = S.col2\n",
    "WHEN NOT MATCHED THEN INSERT (key, col1, col2) VALUES (S.key, S.col1, S.col2)\n",
    "WHEN NOT MATCHED BY SOURCE AND T.update_source = 'external' THEN DELETE;\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- `MERGE` supports `WHEN NOT MATCHED BY SOURCE` to delete rows that no longer exist upstream; use carefully to avoid accidental data loss.\n",
    "\n",
    "---\n",
    "\n",
    "## Q14 — Implement GDPR-style soft deletion using masking + time ranges\n",
    "\n",
    "**Problem**\n",
    "- Support soft-deletion for PII data and automatically mask values for deleted users while retaining analytics.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "**Approach:**\n",
    "- Add `deleted_at` timestamp column. Use conditional masking in views: if `deleted_at` IS NOT NULL then mask PII columns using `HASH()` or `NULL`.\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE VIEW customer_view AS\n",
    "SELECT id,\n",
    "       CASE WHEN deleted_at IS NULL THEN email ELSE NULL END AS email,\n",
    "       CASE WHEN deleted_at IS NULL THEN ssn ELSE 'REDACTED' END AS ssn,\n",
    "       deleted_at\n",
    "FROM customer;\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- Views provide controlled access—masking PII while retaining row-level metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Q15 — Use materialized view to accelerate frequent aggregation with freshness constraints\n",
    "\n",
    "**Problem**\n",
    "- A dashboard queries daily aggregates frequently; reduce latency while keeping results near-real-time.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "```sql\n",
    "CREATE MATERIALIZED VIEW mv_daily_sales\n",
    "CLUSTER BY (sale_date)\n",
    "AS\n",
    "SELECT sale_date, SUM(amount) AS total_sales, COUNT(*) AS tx_count\n",
    "FROM sales\n",
    "GROUP BY sale_date;\n",
    "\n",
    "-- Refresh occurs automatically; monitor via ACCOUNT_USAGE.MATERIALIZED_VIEWS\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- Materialized views precompute aggregates; they trade storage for query latency. Monitor maintenance cost and invalidation frequency.\n",
    "\n",
    "---\n",
    "\n",
    "File created: `snowflake_medium_questions.md`\n",
    "\n",
    "If you want, I can also:\n",
    "- Convert this to a notebook cell, or\n",
    "- Attempt syntactic validation of these SQL snippets if you provide Snowflake credentials."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
